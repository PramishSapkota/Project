{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMC96CT63Ear"
      },
      "source": [
        "Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ygcbUQ962-E-",
        "outputId": "97f01145-8c3f-4ac1-822b-a37ad0d3a355"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers datasets torch pandas scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db5WAzsDQNGQ"
      },
      "source": [
        "Loading and Preprocessing our Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RXtJFtSw3PdD",
        "outputId": "29d6d484-c3df-4bfe-d0db-099aec857694"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer,logging\n",
        "logging.set_verbosity_error()  # Suppress logs\n",
        "\n",
        "# Load CSV file\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/translated_4000_reviews.csv\")\n",
        "\n",
        "# Check for missing values\n",
        "df = df.dropna()\n",
        "df['label'] = df['label'].apply(lambda x: 1 if x == 'OR' else 0)\n",
        "df.head()\n",
        "\n",
        "# Convert ratings to string and concatenate with review text\n",
        "df[\"combined_text\"] = df[\"rating\"].astype(str) + \" [SEP] \" + df[\"Nepali Review\"]\n",
        "\n",
        "\n",
        "# Splitting dataset into train & test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df[\"combined_text\"].tolist(), df[\"label\"].tolist(), test_size=0.2, random_state=1)\n",
        "\n",
        "# Load NepBERT tokenizer\n",
        "model_name = \"NepBERTa/NepBERTa\"  # Change if you're using another Nepali BERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize the text\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0ERrhe33ToD"
      },
      "source": [
        "Convert Data to PyTorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F0g7n4vb3ROj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class ReviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "# Convert to PyTorch dataset\n",
        "train_dataset = ReviewDataset(train_encodings, train_labels)\n",
        "test_dataset = ReviewDataset(test_encodings, test_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fhbqY2KQdJA"
      },
      "source": [
        "Loading Pre-trained NepBERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Od7_THo3Xfj",
        "outputId": "76f8b63c-d10b-4910-c26d-40d7472463df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All TF 2.0 model weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the TF 2.0 model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Load NepBERT with 2 output labels (Fake vs. Real)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, from_tf=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8mwp8zaQrzO"
      },
      "source": [
        "Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "_JhQ9bpT3nr1",
        "outputId": "e2bf05ad-eb1a-476a-ffcf-c6ceb64a06c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1600/1600 03:53, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.779361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.206400</td>\n",
              "      <td>0.557385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.206400</td>\n",
              "      <td>0.700805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.069600</td>\n",
              "      <td>0.897976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.033300</td>\n",
              "      <td>0.846605</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1600, training_loss=0.09680771628394723, metrics={'train_runtime': 233.9822, 'train_samples_per_second': 68.381, 'train_steps_per_second': 6.838, 'total_flos': 1052444221440000.0, 'train_loss': 0.09680771628394723, 'epoch': 5.0})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Disable W&B if not using it\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Define training parameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",  # Updated from evaluation_strategy\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=10,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    fp16=True,\n",
        "    # gradient_accumulation_steps=4,  # Accumulates gradients over 4 steps\n",
        ")\n",
        "\n",
        "\n",
        "# Use Trainer API to fine-tune NepBERT\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU0ibmc1Q0k3"
      },
      "source": [
        "Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uicNQMUh3sr8",
        "outputId": "413353e5-b4b8-4b75-d6cd-7053db63dc5c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 0.87\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Get predictions\n",
        "preds = trainer.predict(test_dataset)\n",
        "pred_labels = torch.argmax(torch.tensor(preds.predictions), axis=1).numpy()\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(test_labels, pred_labels)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oTdq4QiQ8_u"
      },
      "source": [
        "**Save and Test the Model**\n",
        "\n",
        "Using \"save_pretrained\" which is the preferred Hugging Face's method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lrijfgz6B80U"
      },
      "outputs": [],
      "source": [
        "# Save model using Hugging Face's method\n",
        "model.save_pretrained(\"./fine_tuned_nepbert\")\n",
        "\n",
        "# Load model using Hugging Face's method\n",
        "# from transformers import BertForSequenceClassification, BertTokenizer\n",
        "# model = BertForSequenceClassification.from_pretrained(\"./fine_tuned_nepbert\")\n",
        "# tokenizer = BertTokenizer.from_pretrained(\"./fine_tuned_nepbert\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T_i6azkRI_m"
      },
      "source": [
        "Using pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BZO2bY1KCGEq"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save the model to a pickle file\n",
        "with open(\"nepbert_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(model, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNo4jwVNRZrG"
      },
      "source": [
        "Checking Model output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0giq0LfFSWD",
        "outputId": "12823d05-ffd0-4cb6-ea97-c027d3284c87"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=\"./fine_tuned_nepbert\", tokenizer=tokenizer)\n",
        "# or\n",
        "# Load model using Hugging Face's method\n",
        "# from transformers import BertForSequenceClassification, BertTokenizer\n",
        "# model = BertForSequenceClassification.from_pretrained(\"./fine_tuned_nepbert\")\n",
        "# tokenizer = BertTokenizer.from_pretrained(\"./fine_tuned_nepbert\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeB8EQmq70Ta",
        "outputId": "df9f37ec-bfbc-4620-8135-5470bf383c30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review is Fake with 99.84% accuracy.\n"
          ]
        }
      ],
      "source": [
        "# Example rating and review\n",
        "rating = 1\n",
        "review = \"यो होटल ठगि हो।\"  # Example Nepali review\n",
        "\n",
        "# Concatenate rating with review (same format as training)\n",
        "input_text = f\"{rating} [SEP] {review}\"\n",
        "\n",
        "# Get prediction\n",
        "result = classifier(input_text)\n",
        "\n",
        "# print(result) #gives following o/p\n",
        "# [{'label': 'LABEL_0', 'score': 0.9900834560394287}]\n",
        "\n",
        "# Extract label and confidence score\n",
        "label = result[0]['label']\n",
        "score = result[0]['score'] * 100  # Convert to percentage\n",
        "\n",
        "# Interpret the label (assuming LABEL_0 = Fake, LABEL_1 = Real)\n",
        "label_text = \"Fake\" if label == \"LABEL_0\" else \"Real\"\n",
        "\n",
        "# Display output\n",
        "print(f\"Review is {label_text} with {score:.2f}% accuracy.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYIK638GyI7S",
        "outputId": "16460fe0-be82-4a5e-e439-062711894f8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review is Fake with 99.98% accuracy.\n"
          ]
        }
      ],
      "source": [
        "# Example rating and review\n",
        "rating = 5\n",
        "review = \"यसले मेरो फोनलाई चार्जर पूर्ण रूपमा चार्ज भएपछि मात्र चार्ज गर्न अनुमति दिन्छ। यसको एक मात्र नकारात्मक पक्ष भनेको जब तपाईं\"\n",
        "\n",
        "# Concatenate rating with review (same format as training)\n",
        "input_text = f\"{rating} [SEP] {review}\"\n",
        "\n",
        "# Get prediction\n",
        "result = classifier(input_text)\n",
        "\n",
        "# Extract label and confidence score\n",
        "label = result[0]['label']\n",
        "score = result[0]['score'] * 100  # Convert to percentage\n",
        "\n",
        "# Interpret the label (assuming LABEL_0 = Fake, LABEL_1 = Real)\n",
        "label_text = \"Fake\" if label == \"LABEL_0\" else \"Real\"\n",
        "\n",
        "# Display output\n",
        "print(f\"Review is {label_text} with {score:.2f}% accuracy.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
