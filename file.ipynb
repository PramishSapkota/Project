{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received Review: \n",
      "Received Rating: 0\n"
     ]
    }
   ],
   "source": [
    "nr_input_review = globals().get('nr_input_review', \"\")  # Default to empty string if not provided\n",
    "nr_input_rating = globals().get('nr_input_rating', 0.0)  # Default to 0.0 if not provided\n",
    "\n",
    "nr_input_rating = int(nr_input_rating)  \n",
    "nr_input_review = str(nr_input_review) \n",
    "\n",
    "# Make sure the nrs are properly received\n",
    "print(f\"Received Review: {nr_input_review}\")\n",
    "print(f\"Received Rating: {nr_input_rating}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Review: \n",
      "Processing Rating: 0\n"
     ]
    }
   ],
   "source": [
    "import papermill as pm\n",
    "import sys\n",
    "\n",
    "# Read input parameters\n",
    "try:\n",
    "    nr_input_review = nr_input_review  # This comes from papermill\n",
    "    nr_input_rating = nr_input_rating\n",
    "except NameError:\n",
    "    nr_input_review = \"Default review\"\n",
    "    nr_input_rating = 5 # Default values for testing\n",
    "\n",
    "print(f\"Processing Review: {nr_input_review}\")\n",
    "print(f\"Processing Rating: {nr_input_rating}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_temp_review = nr_input_review\n",
    "nr_temp_rating = nr_input_rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Review Predictions\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Review Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load Saved Models\n",
    "\n",
    "import joblib\n",
    "import fasttext\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "try:\n",
    "    print(\"Loading models...\")\n",
    "    svm_model = joblib.load(\"roman_svm_model.pkl\")\n",
    "    tfidf_vectorizer = joblib.load(\"roman_tfidf_vectorizer.pkl\")\n",
    "    word2vec_model = joblib.load(\"roman_word2vec_model.pkl\")\n",
    "    ft_model = fasttext.load_model(\"roman_fasttext_model.bin\")\n",
    "    rf_model = joblib.load(\"roman_rf_model.pkl\")\n",
    "    lr_model = joblib.load(\"roman_lr_model.pkl\")\n",
    "    \n",
    "    \n",
    "    print(\"Models loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\achyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\achyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\achyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_input_review = nr_input_review.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_input_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    \n",
    "    text = re.sub(r'\\bu\\b', 'timi', text)  \n",
    "    text = re.sub(r'\\bm\\b', 'ma', text)\n",
    "    text = re.sub(r'\\beka\\b', 'ek', text)\n",
    "\n",
    "    # Remove extra spaces and normalize spacing\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "nr_input_review = normalize_text(nr_input_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_input_review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'nga', '', text)\n",
    "     \n",
    "    return text\n",
    "\n",
    "nr_input_review = remove_noise(nr_input_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_input_review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def convert_emojis_to_text(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "\n",
    "nr_input_review = convert_emojis_to_text(nr_input_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_input_review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_slang(text):\n",
    "    slang_dict = {'thikkk': 'thik', \n",
    "        'ghamta': 'samta', \n",
    "        'farkera': 'pachhi',\n",
    "        'xa': 'cha',\n",
    "        'hoina': 'haina',\n",
    "        'k': 'ke',\n",
    "        'khoi': 'kahaan',\n",
    "        'kati': 'kati',\n",
    "        'k garne': 'ke garne',\n",
    "        'thaxa': 'thaha',\n",
    "        'thaxaina': 'thaha chaina',\n",
    "        'kya': 'kya ho',\n",
    "        'la': 'la',\n",
    "        'hait': 'hait',\n",
    "        'dherai': 'dherai',\n",
    "        'ali': 'ali',\n",
    "        'kasto': 'kasto',\n",
    "        'k cha': 'ke cha',\n",
    "        'kura': 'kura',\n",
    "        'khate': 'khate',\n",
    "        'dai': 'dai',\n",
    "        'didi': 'didi',\n",
    "        'bhai': 'bhai',\n",
    "        'bahini': 'bahini',\n",
    "        'muji': 'muji',\n",
    "        'kukur': 'kukur',\n",
    "        'jasto': 'jasto',\n",
    "        'testo': 'testo',\n",
    "        'yesto': 'yesto',\n",
    "        'kina': 'kina',\n",
    "        'huncha': 'huncha',\n",
    "        'hunna': 'hunna',\n",
    "        'pugyo': 'pugyo',\n",
    "        'pugena': 'pugena',\n",
    "        'khaana': 'khaana',\n",
    "        'khayo': 'khayo',\n",
    "        'khana': 'khana',\n",
    "        'bas': 'bas',\n",
    "        'chhito': 'chhito',\n",
    "        'bholi': 'bholi',\n",
    "        'aaja': 'aaja',\n",
    "        'parla': 'parla',\n",
    "        'pardaina': 'pardaina',\n",
    "        'thik': 'thik',\n",
    "        'thikai': 'thikai',\n",
    "        'ramro': 'ramro',\n",
    "        'naramro': 'naramro',\n",
    "        'khatra': 'khatra',\n",
    "        'halka': 'halka',\n",
    "        'maile': 'maile',\n",
    "        'timi': 'timi',\n",
    "        'huss': 'huss',\n",
    "        'guff': 'guff',\n",
    "        'jhyau': 'jhyau',\n",
    "        'khuro': 'khuro',\n",
    "        'lado': 'lado',\n",
    "        'thulo': 'thulo',\n",
    "        'sano': 'sano',\n",
    "        'khaire': 'khaire',\n",
    "        'jholey': 'jholey',\n",
    "        'fuchhey': 'fuchhey',\n",
    "        'khatey': 'khatey',\n",
    "        'boka': 'boka',\n",
    "        'bokey': 'bokey',\n",
    "        'bokeycha': 'bokeycha',\n",
    "        'bokeko': 'bokeko',\n",
    "        'bokera': 'bokera',\n",
    "        'bokne': 'bokne',\n",
    "        'boknu': 'boknu',\n",
    "        'boknus': 'boknus',\n",
    "        'boknuparne': 'boknuparne',\n",
    "        'boknuparyo': 'boknuparyo',\n",
    "        'boknuparcha': 'boknuparcha',\n",
    "        'xa' : 'cha',\n",
    "        'khai': 'malai tha xaina',\n",
    "        'gr8': 'great',\n",
    "        'bro': 'bhai',\n",
    "        'thik xa': 'thik cha',\n",
    "        'k xa': 'ke cha',\n",
    "        'momo': 'dumpling',\n",
    "    }\n",
    "    for slang, standard in slang_dict.items():\n",
    "        text = text.replace(slang, standard)\n",
    "    return text\n",
    "\n",
    "nr_input_review = handle_slang(nr_input_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_input_review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['ra', 'ko', 'le', 'lai', 'bata', 'xa', 'yo', 'tiyo', 'mero', 'maile', 'ma', 'lagi', 'mana', 'malai', 'ho', 'tara', 'pani',  'chan', 'garna', 'hunxa', 'of', 'a', 'an', 'the', 'is', 'and', 'but' ]\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "nr_input_review = remove_stopwords(nr_input_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_input_review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "nr_input_review = tokenize(nr_input_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_input_review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "lemmatizer_dict = {\n",
    "    'gardaichha': 'garnu',\n",
    "    'garchha': 'garnu',\n",
    "    'garera': 'garnu',\n",
    "    'garne': 'garnu',\n",
    "    'bhayeko': 'bhayeko',\n",
    "    'jane': 'jan',\n",
    "    'huncha': 'hunu',\n",
    "    'hune': 'hunu',\n",
    "    'pugne': 'pugnu',\n",
    "    'garne': 'garnu',\n",
    "    'chha': 'cha',\n",
    "    'aune': 'aunu',\n",
    "    'jane': 'jan',\n",
    "    'dekhe': 'dekhnus',\n",
    "    'garaune': 'garnu',\n",
    "    'jaane': 'jan'\n",
    "}\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    \n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)  \n",
    "\n",
    "\n",
    "    words = text.split()\n",
    "    lemmatized_words = []\n",
    "\n",
    "    for word in words:\n",
    "        \n",
    "        if word in lemmatizer_dict:\n",
    "            lemmatized_words.append(lemmatizer_dict[word])\n",
    "        else:\n",
    "            lemmatized_words.append(word)\n",
    "\n",
    "    \n",
    "    return ' '.join(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_input_review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word2vec(text):\n",
    "    words = text.split()\n",
    "    known_words = [word for word in words if word in word2vec_model.wv]\n",
    "    if not known_words:\n",
    "        return np.zeros(100)\n",
    "    return np.mean([word2vec_model.wv[word] for word in known_words], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Convert Text to FastText Features\n",
    "def average_fasttext(text):\n",
    "    words = text.split()\n",
    "    vectors = [ft_model.get_word_vector(word) for word in words if word in ft_model.words]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(300)  # Zero vector if no words match\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Predict Review Authenticity\n",
    "def predict_review(nr_input_review):\n",
    "    try:\n",
    "        print(f\"\\nOriginal Input: {nr_temp_review}\")\n",
    "        print(f\"Review: {nr_temp_rating}\\n\")\n",
    "\n",
    "        # Step 1: Preprocess Text\n",
    "        processed_text = lemmatize(nr_input_review)\n",
    "        \n",
    "\n",
    "        # Step 2: Extract Features\n",
    "        tfidf_features = tfidf_vectorizer.transform([processed_text])\n",
    "        \n",
    "\n",
    "        word2vec_features = np.array([average_word2vec(processed_text)])\n",
    "        \n",
    "\n",
    "        fasttext_features = np.array([average_fasttext(processed_text)])\n",
    "        \n",
    "\n",
    "        # Step 3: Combine Features\n",
    "        try:\n",
    "            combined_features = hstack([tfidf_features, word2vec_features, fasttext_features])\n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature stacking: {e}\")\n",
    "            return\n",
    "\n",
    "        # Step 4: Make Prediction\n",
    "        try:\n",
    "            prediction = int(svm_model.predict(combined_features)[0])\n",
    "        except Exception as e:\n",
    "            print(f\"Error in prediction: {e}\")\n",
    "            return\n",
    "\n",
    "        # Step 5: Get Probability Score (if available)\n",
    "        try:\n",
    "            probability = svm_model.predict_proba(combined_features)[0][prediction]\n",
    "            print(f\"Predicted Review Authenticity: {'REAL' if prediction == 1 else 'FAKE'} (Confidence: {probability:.2f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in probability calculation: {e}\")\n",
    "            probability = None  # Some models donâ€™t support predict_proba()\n",
    "\n",
    "        return \"REAL\" if prediction == 1 else \"FAKE\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Input: \n",
      "Rating: 0\n",
      "\n",
      "Error in probability calculation: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Final Prediction: REAL\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = predict_review(' '.join(nr_input_review))  # Ensure input is a string\n",
    "print(f\"Final Prediction: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
